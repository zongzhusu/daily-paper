{
  "mode": "score_v0",
  "items": [
    {
      "id": "arxiv:2602.18435v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18435v1",
      "title": "Assigning Confidence: K-partition Ensembles",
      "abstract": "Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.",
      "content": null,
      "authors": [
        "Aggelos Semoglou",
        "John Pavlopoulos"
      ],
      "publishedAt": "2026-02-20T18:59:53Z",
      "categories": [
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18435v1",
        "title": "Assigning Confidence: K-partition Ensembles",
        "updated": "2026-02-20T18:59:53Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18435v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18435v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.",
        "category": {
          "term": "cs.LG",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T18:59:53Z",
        "arxiv:comment": "31 pages including appendix",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Aggelos Semoglou",
            "#text": "\n      \n    "
          },
          {
            "name": "John Pavlopoulos",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18429v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18429v1",
      "title": "VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning",
      "abstract": "Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.",
      "content": null,
      "authors": [
        "Harshul Raj Surana",
        "Arijit Maji",
        "Aryan Vats",
        "Akash Ghosh",
        "Sriparna Saha",
        "Amit Sheth"
      ],
      "publishedAt": "2026-02-20T18:53:07Z",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18429v1",
        "title": "VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning",
        "updated": "2026-02-20T18:53:07Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18429v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18429v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.",
        "category": [
          {
            "term": "cs.CL",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.IR",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T18:53:07Z",
        "arxiv:primary_category": {
          "term": "cs.CL"
        },
        "author": [
          {
            "name": "Harshul Raj Surana",
            "#text": "\n      \n    "
          },
          {
            "name": "Arijit Maji",
            "#text": "\n      \n    "
          },
          {
            "name": "Aryan Vats",
            "#text": "\n      \n    "
          },
          {
            "name": "Akash Ghosh",
            "#text": "\n      \n    "
          },
          {
            "name": "Sriparna Saha",
            "#text": "\n      \n    "
          },
          {
            "name": "Amit Sheth",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18428v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18428v1",
      "title": "The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning",
      "abstract": "Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\\text{marg}}(\\mathbf{u}) = -\\log p(\\mathbf{u})$, where $p(\\mathbf{u}) = \\int p(\\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.",
      "content": null,
      "authors": [
        "Mojtaba Sahraee-Ardakan",
        "Mauricio Delbracio",
        "Peyman Milanfar"
      ],
      "publishedAt": "2026-02-20T18:49:00Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "eess.IV"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18428v1",
        "title": "The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning",
        "updated": "2026-02-20T18:49:00Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18428v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18428v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\\text{marg}}(\\mathbf{u}) = -\\log p(\\mathbf{u})$, where $p(\\mathbf{u}) = \\int p(\\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.CV",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "eess.IV",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T18:49:00Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Mojtaba Sahraee-Ardakan",
            "#text": "\n      \n    "
          },
          {
            "name": "Mauricio Delbracio",
            "#text": "\n      \n    "
          },
          {
            "name": "Peyman Milanfar",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18425v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18425v1",
      "title": "RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering",
      "abstract": "Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.",
      "content": null,
      "authors": [
        "Deniz Qian",
        "Hung-Ting Chen",
        "Eunsol Choi"
      ],
      "publishedAt": "2026-02-20T18:48:05Z",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18425v1",
        "title": "RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering",
        "updated": "2026-02-20T18:48:05Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18425v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18425v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.",
        "category": [
          {
            "term": "cs.CL",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.IR",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T18:48:05Z",
        "arxiv:comment": "18 pages, 12 figures, 12 tables",
        "arxiv:primary_category": {
          "term": "cs.CL"
        },
        "author": [
          {
            "name": "Deniz Qian",
            "#text": "\n      \n    "
          },
          {
            "name": "Hung-Ting Chen",
            "#text": "\n      \n    "
          },
          {
            "name": "Eunsol Choi",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18420v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18420v1",
      "title": "SPQ: An Ensemble Technique for Large Language Model Compression",
      "abstract": "This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/",
      "content": null,
      "authors": [
        "Jiamin Yao",
        "Eren Gultepe"
      ],
      "publishedAt": "2026-02-20T18:44:16Z",
      "categories": [
        "cs.CL"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18420v1",
        "title": "SPQ: An Ensemble Technique for Large Language Model Compression",
        "updated": "2026-02-20T18:44:16Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18420v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18420v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/",
        "category": {
          "term": "cs.CL",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T18:44:16Z",
        "arxiv:comment": "Accepted to LREC 2026 Main Conference",
        "arxiv:primary_category": {
          "term": "cs.CL"
        },
        "author": [
          {
            "name": "Jiamin Yao",
            "#text": "\n      \n    "
          },
          {
            "name": "Eren Gultepe",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18419v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18419v1",
      "title": "Benchmarking Graph Neural Networks in Solving Hard Constraint Satisfaction Problems",
      "abstract": "Graph neural networks (GNNs) are increasingly applied to hard optimization problems, often claiming superiority over classical heuristics. However, such claims risk being unsolid due to a lack of standard benchmarks on truly hard instances. From a statistical physics perspective, we propose new hard benchmarks based on random problems. We provide these benchmarks, along with performance results from both classical heuristics and GNNs. Our fair comparison shows that classical algorithms still outperform GNNs. We discuss the challenges for neural networks in this domain. Future claims of superiority can be made more robust using our benchmarks, available at https://github.com/ArtLabBocconi/RandCSPBench.",
      "content": null,
      "authors": [
        "Geri Skenderi",
        "Lorenzo Buffoni",
        "Francesco D'Amico",
        "David Machado",
        "Raffaele Marino",
        "Matteo Negri",
        "Federico Ricci-Tersenghi",
        "Carlo Lucibello",
        "Maria Chiara Angelini"
      ],
      "publishedAt": "2026-02-20T18:41:48Z",
      "categories": [
        "cond-mat.dis-nn",
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18419v1",
        "title": "Benchmarking Graph Neural Networks in Solving Hard Constraint Satisfaction Problems",
        "updated": "2026-02-20T18:41:48Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18419v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18419v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Graph neural networks (GNNs) are increasingly applied to hard optimization problems, often claiming superiority over classical heuristics. However, such claims risk being unsolid due to a lack of standard benchmarks on truly hard instances. From a statistical physics perspective, we propose new hard benchmarks based on random problems. We provide these benchmarks, along with performance results from both classical heuristics and GNNs. Our fair comparison shows that classical algorithms still outperform GNNs. We discuss the challenges for neural networks in this domain. Future claims of superiority can be made more robust using our benchmarks, available at https://github.com/ArtLabBocconi/RandCSPBench.",
        "category": [
          {
            "term": "cond-mat.dis-nn",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T18:41:48Z",
        "arxiv:primary_category": {
          "term": "cond-mat.dis-nn"
        },
        "author": [
          {
            "name": "Geri Skenderi",
            "#text": "\n      \n    "
          },
          {
            "name": "Lorenzo Buffoni",
            "#text": "\n      \n    "
          },
          {
            "name": "Francesco D'Amico",
            "#text": "\n      \n    "
          },
          {
            "name": "David Machado",
            "#text": "\n      \n    "
          },
          {
            "name": "Raffaele Marino",
            "#text": "\n      \n    "
          },
          {
            "name": "Matteo Negri",
            "#text": "\n      \n    "
          },
          {
            "name": "Federico Ricci-Tersenghi",
            "#text": "\n      \n    "
          },
          {
            "name": "Carlo Lucibello",
            "#text": "\n      \n    "
          },
          {
            "name": "Maria Chiara Angelini",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18417v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18417v1",
      "title": "Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures",
      "abstract": "This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.",
      "content": null,
      "authors": [
        "Joshua Nunley"
      ],
      "publishedAt": "2026-02-20T18:35:43Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18417v1",
        "title": "Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures",
        "updated": "2026-02-20T18:35:43Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18417v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18417v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.CL",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T18:35:43Z",
        "arxiv:comment": "12 pages, 3 figures, 8 tables",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": {
          "name": "Joshua Nunley",
          "#text": "\n      \n    "
        },
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18409v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18409v1",
      "title": "Unifying approach to uniform expressivity of graph neural networks",
      "abstract": "The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.",
      "content": null,
      "authors": [
        "Huan Luo",
        "Jonni Virtema"
      ],
      "publishedAt": "2026-02-20T18:18:48Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18409v1",
        "title": "Unifying approach to uniform expressivity of graph neural networks",
        "updated": "2026-02-20T18:18:48Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18409v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18409v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LO",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T18:18:48Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Huan Luo",
            "#text": "\n      \n    "
          },
          {
            "name": "Jonni Virtema",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18406v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18406v1",
      "title": "Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges",
      "abstract": "Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.",
      "content": null,
      "authors": [
        "Minh Dinh",
        "Stéphane Deny"
      ],
      "publishedAt": "2026-02-20T18:14:05Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18406v1",
        "title": "Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges",
        "updated": "2026-02-20T18:14:05Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18406v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18406v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.",
        "category": [
          {
            "term": "cs.CV",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T18:14:05Z",
        "arxiv:primary_category": {
          "term": "cs.CV"
        },
        "author": [
          {
            "name": "Minh Dinh",
            "#text": "\n      \n    "
          },
          {
            "name": "Stéphane Deny",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18403v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18403v1",
      "title": "Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study",
      "abstract": "Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.",
      "content": null,
      "authors": [
        "Orfeas Bourchas",
        "George Papalambrou"
      ],
      "publishedAt": "2026-02-20T18:12:14Z",
      "categories": [
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18403v1",
        "title": "Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study",
        "updated": "2026-02-20T18:12:14Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18403v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18403v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.",
        "category": {
          "term": "cs.LG",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T18:12:14Z",
        "arxiv:comment": "Accepted to the KGML Bridge at AAAI 2026 (non-archival)",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Orfeas Bourchas",
            "#text": "\n      \n    "
          },
          {
            "name": "George Papalambrou",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18401v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18401v1",
      "title": "Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay",
      "abstract": "Biological neural networks (like the hippocampus) can internally generate \"replay\" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.",
      "content": null,
      "authors": [
        "Josue Casco-Rodriguez",
        "Nanda H. Krishna",
        "Richard G. Baraniuk"
      ],
      "publishedAt": "2026-02-20T18:07:09Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "stat.ML"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18401v1",
        "title": "Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay",
        "updated": "2026-02-20T18:07:09Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18401v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18401v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Biological neural networks (like the hippocampus) can internally generate \"replay\" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "q-bio.NC",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "stat.ML",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T18:07:09Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Josue Casco-Rodriguez",
            "#text": "\n      \n    "
          },
          {
            "name": "Nanda H. Krishna",
            "#text": "\n      \n    "
          },
          {
            "name": "Richard G. Baraniuk",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18396v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18396v1",
      "title": "PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing",
      "abstract": "We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.",
      "content": null,
      "authors": [
        "Ehsan Lari",
        "Reza Arablouei",
        "Stefan Werner"
      ],
      "publishedAt": "2026-02-20T18:01:59Z",
      "categories": [
        "cs.LG",
        "eess.SP",
        "math.PR",
        "stat.AP",
        "stat.ML"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18396v1",
        "title": "PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing",
        "updated": "2026-02-20T18:01:59Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18396v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18396v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "eess.SP",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "math.PR",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "stat.AP",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "stat.ML",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T18:01:59Z",
        "arxiv:comment": "13 pages, 5 figures, 2 tables, Submitted to IEEE Transactions on Signal Processing (TSP)",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Ehsan Lari",
            "#text": "\n      \n    "
          },
          {
            "name": "Reza Arablouei",
            "#text": "\n      \n    "
          },
          {
            "name": "Stefan Werner",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18386v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18386v1",
      "title": "Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO",
      "abstract": "Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.",
      "content": null,
      "authors": [
        "Mohamed Elgouhary",
        "Amr S. El-Wakeel"
      ],
      "publishedAt": "2026-02-20T17:48:21Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18386v1",
        "title": "Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO",
        "updated": "2026-02-20T17:48:21Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18386v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18386v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.",
        "category": [
          {
            "term": "cs.RO",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "eess.SY",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T17:48:21Z",
        "arxiv:primary_category": {
          "term": "cs.RO"
        },
        "author": [
          {
            "name": "Mohamed Elgouhary",
            "#text": "\n      \n    "
          },
          {
            "name": "Amr S. El-Wakeel",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18384v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18384v1",
      "title": "FedZMG: Efficient Client-Side Optimization in Federated Learning",
      "abstract": "Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the \"intensity\" or \"bias\" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.",
      "content": null,
      "authors": [
        "Fotios Zantalis",
        "Evangelos Zervas",
        "Grigorios Koulouras"
      ],
      "publishedAt": "2026-02-20T17:45:28Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18384v1",
        "title": "FedZMG: Efficient Client-Side Optimization in Federated Learning",
        "updated": "2026-02-20T17:45:28Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18384v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18384v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the \"intensity\" or \"bias\" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T17:45:28Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Fotios Zantalis",
            "#text": "\n      \n    "
          },
          {
            "name": "Evangelos Zervas",
            "#text": "\n      \n    "
          },
          {
            "name": "Grigorios Koulouras",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18377v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18377v1",
      "title": "Theory and interpretability of Quantum Extreme Learning Machines: a Pauli-transfer matrix approach",
      "abstract": "Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to train. Here, we consider n-qubit quantum extreme learning machines (QELMs) with continuous-time reservoir dynamics. QELMs are memoryless QRCs capable of various ML tasks, including image classification and time series forecasting. We apply the Pauli transfer matrix (PTM) formalism to theoretically analyze the influence of encoding, reservoir dynamics, and measurement operations, including temporal multiplexing, on the QELM performance. This formalism makes explicit that the encoding determines the complete set of (nonlinear) features available to the QELM, while the quantum channels linearly transform these features before they are probed by the chosen measurement operators. Optimizing a QELM can therefore be cast as a decoding problem in which one shapes the channel-induced transformations such that task-relevant features become available to the regressor. The PTM formalism allows one to identify the classical representation of a QELM and thereby guide its design towards a given training objective. As a specific application, we focus on learning nonlinear dynamical systems and show that a QELM trained on such trajectories learns a surrogate-approximation to the underlying flow map.",
      "content": null,
      "authors": [
        "Markus Gross",
        "Hans-Martin Rieser"
      ],
      "publishedAt": "2026-02-20T17:33:27Z",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18377v1",
        "title": "Theory and interpretability of Quantum Extreme Learning Machines: a Pauli-transfer matrix approach",
        "updated": "2026-02-20T17:33:27Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18377v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18377v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Quantum reservoir computers (QRCs) have emerged as a promising approach to quantum machine learning, since they utilize the natural dynamics of quantum systems for data processing and are simple to train. Here, we consider n-qubit quantum extreme learning machines (QELMs) with continuous-time reservoir dynamics. QELMs are memoryless QRCs capable of various ML tasks, including image classification and time series forecasting. We apply the Pauli transfer matrix (PTM) formalism to theoretically analyze the influence of encoding, reservoir dynamics, and measurement operations, including temporal multiplexing, on the QELM performance. This formalism makes explicit that the encoding determines the complete set of (nonlinear) features available to the QELM, while the quantum channels linearly transform these features before they are probed by the chosen measurement operators. Optimizing a QELM can therefore be cast as a decoding problem in which one shapes the channel-induced transformations such that task-relevant features become available to the regressor. The PTM formalism allows one to identify the classical representation of a QELM and thereby guide its design towards a given training objective. As a specific application, we focus on learning nonlinear dynamical systems and show that a QELM trained on such trajectories learns a surrogate-approximation to the underlying flow map.",
        "category": [
          {
            "term": "quant-ph",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T17:33:27Z",
        "arxiv:comment": "34 pages, 12 figures",
        "arxiv:primary_category": {
          "term": "quant-ph"
        },
        "author": [
          {
            "name": "Markus Gross",
            "#text": "\n      \n    "
          },
          {
            "name": "Hans-Martin Rieser",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18374v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18374v1",
      "title": "Zero-shot Interactive Perception",
      "abstract": "Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.",
      "content": null,
      "authors": [
        "Venkatesh Sripada",
        "Frank Guerin",
        "Amir Ghalamzan"
      ],
      "publishedAt": "2026-02-20T17:30:25Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18374v1",
        "title": "Zero-shot Interactive Perception",
        "updated": "2026-02-20T17:30:25Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18374v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18374v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.",
        "category": [
          {
            "term": "cs.RO",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T17:30:25Z",
        "arxiv:comment": "Original manuscript submitted on April 24, 2025. Timestamped and publicly available on OpenReview: https://openreview.net/forum?id=7MhpFcr5Nx",
        "arxiv:primary_category": {
          "term": "cs.RO"
        },
        "author": [
          {
            "name": "Venkatesh Sripada",
            "#text": "\n      \n    "
          },
          {
            "name": "Frank Guerin",
            "#text": "\n      \n    "
          },
          {
            "name": "Amir Ghalamzan",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18372v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18372v1",
      "title": "\"How Do I ...?\": Procedural Questions Predominate Student-LLM Chatbot Conversations",
      "abstract": "Providing scaffolding through educational chatbots built on Large Language Models (LLM) has potential risks and benefits that remain an open area of research. When students navigate impasses, they ask for help by formulating impasse-driven questions. Within interactions with LLM chatbots, such questions shape the user prompts and drive the pedagogical effectiveness of the chatbot's response. This paper focuses on such student questions from two datasets of distinct learning contexts: formative self-study, and summative assessed coursework. We analysed 6,113 messages from both learning contexts, using 11 different LLMs and three human raters to classify student questions using four existing schemas. On the feasibility of using LLMs as raters, results showed moderate-to-good inter-rater reliability, with higher consistency than human raters. The data showed that 'procedural' questions predominated in both learning contexts, but more so when students prepare for summative assessment. These results provide a basis on which to use LLMs for classification of student questions. However, we identify clear limitations in both the ability to classify with schemas and the value of doing so: schemas are limited and thus struggle to accommodate the semantic richness of composite prompts, offering only partial understanding the wider risks and benefits of chatbot integration. In the future, we recommend an analysis approach that captures the nuanced, multi-turn nature of conversation, for example, by applying methods from conversation analysis in discursive psychology.",
      "content": null,
      "authors": [
        "Alexandra Neagu",
        "Marcus Messer",
        "Peter Johnson",
        "Rhodri Nelson"
      ],
      "publishedAt": "2026-02-20T17:27:41Z",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18372v1",
        "title": "\"How Do I ...?\": Procedural Questions Predominate Student-LLM Chatbot Conversations",
        "updated": "2026-02-20T17:27:41Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18372v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18372v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Providing scaffolding through educational chatbots built on Large Language Models (LLM) has potential risks and benefits that remain an open area of research. When students navigate impasses, they ask for help by formulating impasse-driven questions. Within interactions with LLM chatbots, such questions shape the user prompts and drive the pedagogical effectiveness of the chatbot's response. This paper focuses on such student questions from two datasets of distinct learning contexts: formative self-study, and summative assessed coursework. We analysed 6,113 messages from both learning contexts, using 11 different LLMs and three human raters to classify student questions using four existing schemas. On the feasibility of using LLMs as raters, results showed moderate-to-good inter-rater reliability, with higher consistency than human raters. The data showed that 'procedural' questions predominated in both learning contexts, but more so when students prepare for summative assessment. These results provide a basis on which to use LLMs for classification of student questions. However, we identify clear limitations in both the ability to classify with schemas and the value of doing so: schemas are limited and thus struggle to accommodate the semantic richness of composite prompts, offering only partial understanding the wider risks and benefits of chatbot integration. In the future, we recommend an analysis approach that captures the nuanced, multi-turn nature of conversation, for example, by applying methods from conversation analysis in discursive psychology.",
        "category": [
          {
            "term": "cs.HC",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T17:27:41Z",
        "arxiv:comment": "14 pages, 2 figures",
        "arxiv:primary_category": {
          "term": "cs.HC"
        },
        "author": [
          {
            "name": "Alexandra Neagu",
            "#text": "\n      \n    "
          },
          {
            "name": "Marcus Messer",
            "#text": "\n      \n    "
          },
          {
            "name": "Peter Johnson",
            "#text": "\n      \n    "
          },
          {
            "name": "Rhodri Nelson",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18370v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18370v1",
      "title": "Drawing the LINE: Cryptographic Analysis and Security Improvements for the LINE E2EE Protocol",
      "abstract": "LINE has emerged as one of the most popular communication platforms in many East Asian countries, including Thailand and Japan, with millions of active users. Therefore, it is essential to understand its security guarantees. In this work, we present the first provable security analysis of the LINE version two (LINEv2) messaging protocol, focusing on its cryptographic guarantees in a real-world setting. We capture the architecture and security of the LINE messaging protocol by modifying the Multi-Stage Key Exchange (MSKE) model, a framework for analysing cryptographic protocols under adversarial conditions. While LINEv2 achieves basic security properties such as key indistinguishability and message authentication, we highlight the lack of forward secrecy (FS) and post-compromise security (PCS). To address this, we introduce a stronger version of the LINE protocol, introducing FS and PCS to LINE, analysing and benchmarking our results.",
      "content": null,
      "authors": [
        "Benjamin Dowling",
        "Prosanta Gope",
        "Mehr U Nisa",
        "Bhagya Wimalasiri"
      ],
      "publishedAt": "2026-02-20T17:26:47Z",
      "categories": [
        "cs.CR"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18370v1",
        "title": "Drawing the LINE: Cryptographic Analysis and Security Improvements for the LINE E2EE Protocol",
        "updated": "2026-02-20T17:26:47Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18370v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18370v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "LINE has emerged as one of the most popular communication platforms in many East Asian countries, including Thailand and Japan, with millions of active users. Therefore, it is essential to understand its security guarantees. In this work, we present the first provable security analysis of the LINE version two (LINEv2) messaging protocol, focusing on its cryptographic guarantees in a real-world setting. We capture the architecture and security of the LINE messaging protocol by modifying the Multi-Stage Key Exchange (MSKE) model, a framework for analysing cryptographic protocols under adversarial conditions. While LINEv2 achieves basic security properties such as key indistinguishability and message authentication, we highlight the lack of forward secrecy (FS) and post-compromise security (PCS). To address this, we introduce a stronger version of the LINE protocol, introducing FS and PCS to LINE, analysing and benchmarking our results.",
        "category": {
          "term": "cs.CR",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T17:26:47Z",
        "arxiv:primary_category": {
          "term": "cs.CR"
        },
        "author": [
          {
            "name": "Benjamin Dowling",
            "#text": "\n      \n    "
          },
          {
            "name": "Prosanta Gope",
            "#text": "\n      \n    "
          },
          {
            "name": "Mehr U Nisa",
            "#text": "\n      \n    "
          },
          {
            "name": "Bhagya Wimalasiri",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18364v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18364v1",
      "title": "Quantum Maximum Likelihood Prediction via Hilbert Space Embeddings",
      "abstract": "Recent works have proposed various explanations for the ability of modern large language models (LLMs) to perform in-context prediction. We propose an alternative conceptual viewpoint from an information-geometric and statistical perspective. Motivated by Bach[2023], we model training as learning an embedding of probability distributions into the space of quantum density operators, and in-context learning as maximum-likelihood prediction over a specified class of quantum models. We provide an interpretation of this predictor in terms of quantum reverse information projection and quantum Pythagorean theorem when the class of quantum models is sufficiently expressive. We further derive non-asymptotic performance guarantees in terms of convergence rates and concentration inequalities, both in trace norm and quantum relative entropy. Our approach provides a unified framework to handle both classical and quantum LLMs.",
      "content": null,
      "authors": [
        "Sreejith Sreekumar",
        "Nir Weinberger"
      ],
      "publishedAt": "2026-02-20T17:16:38Z",
      "categories": [
        "cs.IT",
        "cs.LG",
        "quant-ph",
        "stat.ML"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18364v1",
        "title": "Quantum Maximum Likelihood Prediction via Hilbert Space Embeddings",
        "updated": "2026-02-20T17:16:38Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18364v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18364v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Recent works have proposed various explanations for the ability of modern large language models (LLMs) to perform in-context prediction. We propose an alternative conceptual viewpoint from an information-geometric and statistical perspective. Motivated by Bach[2023], we model training as learning an embedding of probability distributions into the space of quantum density operators, and in-context learning as maximum-likelihood prediction over a specified class of quantum models. We provide an interpretation of this predictor in terms of quantum reverse information projection and quantum Pythagorean theorem when the class of quantum models is sufficiently expressive. We further derive non-asymptotic performance guarantees in terms of convergence rates and concentration inequalities, both in trace norm and quantum relative entropy. Our approach provides a unified framework to handle both classical and quantum LLMs.",
        "category": [
          {
            "term": "cs.IT",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "quant-ph",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "stat.ML",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T17:16:38Z",
        "arxiv:comment": "32+4 pages, 1 figure",
        "arxiv:primary_category": {
          "term": "cs.IT"
        },
        "author": [
          {
            "name": "Sreejith Sreekumar",
            "#text": "\n      \n    "
          },
          {
            "name": "Nir Weinberger",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18357v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18357v1",
      "title": "Statistical Confidence in Functional Correctness: An Approach for AI Product Functional Correctness Evaluation",
      "abstract": "The quality assessment of Artificial Intelligence (AI) systems is a fundamental challenge due to their inherently probabilistic nature. Standards such as ISO/IEC 25059 provide a quality model, but they lack practical and statistically robust methods for assessing functional correctness. This paper proposes and evaluates the Statistical Confidence in Functional Correctness (SCFC) approach, which seeks to fill this gap by connecting business requirements to a measure of statistical confidence that considers both the model's average performance and its variability. The approach consists of four steps: defining quantitative specification limits, performing stratified and probabilistic sampling, applying bootstrapping to estimate a confidence interval for the performance metric, and calculating a capability index as a final indicator. The approach was evaluated through a case study on two real-world AI systems in industry involving interviews with AI experts. Valuable insights were collected from the experts regarding the utility, ease of use, and intention to adopt the methodology in practical scenarios. We conclude that the proposed approach is a feasible and valuable way to operationalize the assessment of functional correctness, moving the evaluation from a point estimate to a statement of statistical confidence.",
      "content": null,
      "authors": [
        "Wallace Albertini",
        "Marina Condé Araújo",
        "Júlia Condé Araújo",
        "Antonio Pedro Santos Alves",
        "Marcos Kalinowski"
      ],
      "publishedAt": "2026-02-20T17:06:38Z",
      "categories": [
        "cs.SE"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18357v1",
        "title": "Statistical Confidence in Functional Correctness: An Approach for AI Product Functional Correctness Evaluation",
        "updated": "2026-02-20T17:06:38Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18357v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18357v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "The quality assessment of Artificial Intelligence (AI) systems is a fundamental challenge due to their inherently probabilistic nature. Standards such as ISO/IEC 25059 provide a quality model, but they lack practical and statistically robust methods for assessing functional correctness. This paper proposes and evaluates the Statistical Confidence in Functional Correctness (SCFC) approach, which seeks to fill this gap by connecting business requirements to a measure of statistical confidence that considers both the model's average performance and its variability. The approach consists of four steps: defining quantitative specification limits, performing stratified and probabilistic sampling, applying bootstrapping to estimate a confidence interval for the performance metric, and calculating a capability index as a final indicator. The approach was evaluated through a case study on two real-world AI systems in industry involving interviews with AI experts. Valuable insights were collected from the experts regarding the utility, ease of use, and intention to adopt the methodology in practical scenarios. We conclude that the proposed approach is a feasible and valuable way to operationalize the assessment of functional correctness, moving the evaluation from a point estimate to a statement of statistical confidence.",
        "category": {
          "term": "cs.SE",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T17:06:38Z",
        "arxiv:comment": "Author version of the paper accepted for publication at CAIN 2026",
        "arxiv:primary_category": {
          "term": "cs.SE"
        },
        "author": [
          {
            "name": "Wallace Albertini",
            "#text": "\n      \n    "
          },
          {
            "name": "Marina Condé Araújo",
            "#text": "\n      \n    "
          },
          {
            "name": "Júlia Condé Araújo",
            "#text": "\n      \n    "
          },
          {
            "name": "Antonio Pedro Santos Alves",
            "#text": "\n      \n    "
          },
          {
            "name": "Marcos Kalinowski",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18352v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18352v1",
      "title": "Qualitative Coding Analysis through Open-Source Large Language Models: A User Study and Design Recommendations",
      "abstract": "Qualitative data analysis is labor-intensive, yet the privacy risks associated with commercial Large Language Models (LLMs) often preclude their use in sensitive research. To address this, we introduce ChatQDA, an on-device framework powered by open-source LLMs designed for privacy-preserving open coding. Our mixed-methods user study reveals that while participants rated the system highly for usability and perceived efficiency, they exhibited \"conditional trust\", valuing the tool for surface-level extraction while questioning its interpretive nuance and consistency. Furthermore, despite the technical security of local deployment, participants reported epistemic uncertainty regarding data protection, suggesting that invisible security measures are insufficient to foster trust. We conclude with design recommendations for local-first analysis tools that prioritize verifiable privacy and methodological rigor.",
      "content": null,
      "authors": [
        "Tung T. Ngo",
        "Dai Nguyen Van",
        "Anh-Minh Nguyen",
        "Phuong-Anh Do",
        "Anh Nguyen-Quoc"
      ],
      "publishedAt": "2026-02-20T17:04:02Z",
      "categories": [
        "cs.HC",
        "cs.CR",
        "cs.SE"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18352v1",
        "title": "Qualitative Coding Analysis through Open-Source Large Language Models: A User Study and Design Recommendations",
        "updated": "2026-02-20T17:04:02Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18352v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18352v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Qualitative data analysis is labor-intensive, yet the privacy risks associated with commercial Large Language Models (LLMs) often preclude their use in sensitive research. To address this, we introduce ChatQDA, an on-device framework powered by open-source LLMs designed for privacy-preserving open coding. Our mixed-methods user study reveals that while participants rated the system highly for usability and perceived efficiency, they exhibited \"conditional trust\", valuing the tool for surface-level extraction while questioning its interpretive nuance and consistency. Furthermore, despite the technical security of local deployment, participants reported epistemic uncertainty regarding data protection, suggesting that invisible security measures are insufficient to foster trust. We conclude with design recommendations for local-first analysis tools that prioritize verifiable privacy and methodological rigor.",
        "category": [
          {
            "term": "cs.HC",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.CR",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.SE",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T17:04:02Z",
        "arxiv:comment": "6 pages. Accepted as Poster to CHI'26",
        "arxiv:primary_category": {
          "term": "cs.HC"
        },
        "author": [
          {
            "name": "Tung T. Ngo",
            "#text": "\n      \n    "
          },
          {
            "name": "Dai Nguyen Van",
            "#text": "\n      \n    "
          },
          {
            "name": "Anh-Minh Nguyen",
            "#text": "\n      \n    "
          },
          {
            "name": "Phuong-Anh Do",
            "#text": "\n      \n    "
          },
          {
            "name": "Anh Nguyen-Quoc",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18351v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18351v1",
      "title": "Validating Political Position Predictions of Arguments",
      "abstract": "Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \\textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.",
      "content": null,
      "authors": [
        "Jordan Robinson",
        "Angus R. Williams",
        "Katie Atkinson",
        "Anthony G. Cohn"
      ],
      "publishedAt": "2026-02-20T17:03:44Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18351v1",
        "title": "Validating Political Position Predictions of Arguments",
        "updated": "2026-02-20T17:03:44Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18351v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18351v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \\textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.",
        "category": [
          {
            "term": "cs.CL",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T17:03:44Z",
        "arxiv:comment": "13 pages, 6 figures, 6 tables. Under review",
        "arxiv:primary_category": {
          "term": "cs.CL"
        },
        "author": [
          {
            "name": "Jordan Robinson",
            "#text": "\n      \n    "
          },
          {
            "name": "Angus R. Williams",
            "#text": "\n      \n    "
          },
          {
            "name": "Katie Atkinson",
            "#text": "\n      \n    "
          },
          {
            "name": "Anthony G. Cohn",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18350v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18350v1",
      "title": "Quantum-enhanced satellite image classification",
      "abstract": "We demonstrate the application of a quantum feature extraction method to enhance multi-class image classification for space applications. By harnessing the dynamics of many-body spin Hamiltonians, the method generates expressive quantum features that, when combined with classical processing, lead to quantum-enhanced classification accuracy. Using a strong and well-established ResNet50 baseline, we achieved a maximum classical accuracy of 83%, which can be improved to 84% with a transfer learning approach. In contrast, applying our quantum-classical method the performance is increased to 87% accuracy, demonstrating a clear and reproducible improvement over robust classical approaches. Implemented on several of IBM's quantum processors, our hybrid quantum-classical approach delivers consistent gains of 2-3% in absolute accuracy. These results highlight the practical potential of current and near-term quantum processors in high-stakes, data-driven domains such as satellite imaging and remote sensing, while suggesting broader applicability in real-world machine learning tasks.",
      "content": null,
      "authors": [
        "Qi Zhang",
        "Anton Simen",
        "Carlos Flores-Garrigós",
        "Gabriel Alvarado Barrios",
        "Paolo A. Erdman",
        "Enrique Solano",
        "Aaron C. Kemp",
        "Vincent Beltrani",
        "Vedangi Pathak",
        "Hamed Mohammadbagherpoor"
      ],
      "publishedAt": "2026-02-20T17:02:16Z",
      "categories": [
        "quant-ph",
        "cs.CV",
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18350v1",
        "title": "Quantum-enhanced satellite image classification",
        "updated": "2026-02-20T17:02:16Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18350v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18350v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "We demonstrate the application of a quantum feature extraction method to enhance multi-class image classification for space applications. By harnessing the dynamics of many-body spin Hamiltonians, the method generates expressive quantum features that, when combined with classical processing, lead to quantum-enhanced classification accuracy. Using a strong and well-established ResNet50 baseline, we achieved a maximum classical accuracy of 83%, which can be improved to 84% with a transfer learning approach. In contrast, applying our quantum-classical method the performance is increased to 87% accuracy, demonstrating a clear and reproducible improvement over robust classical approaches. Implemented on several of IBM's quantum processors, our hybrid quantum-classical approach delivers consistent gains of 2-3% in absolute accuracy. These results highlight the practical potential of current and near-term quantum processors in high-stakes, data-driven domains such as satellite imaging and remote sensing, while suggesting broader applicability in real-world machine learning tasks.",
        "category": [
          {
            "term": "quant-ph",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.CV",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T17:02:16Z",
        "arxiv:primary_category": {
          "term": "quant-ph"
        },
        "author": [
          {
            "name": "Qi Zhang",
            "#text": "\n      \n    "
          },
          {
            "name": "Anton Simen",
            "#text": "\n      \n    "
          },
          {
            "name": "Carlos Flores-Garrigós",
            "#text": "\n      \n    "
          },
          {
            "name": "Gabriel Alvarado Barrios",
            "#text": "\n      \n    "
          },
          {
            "name": "Paolo A. Erdman",
            "#text": "\n      \n    "
          },
          {
            "name": "Enrique Solano",
            "#text": "\n      \n    "
          },
          {
            "name": "Aaron C. Kemp",
            "#text": "\n      \n    "
          },
          {
            "name": "Vincent Beltrani",
            "#text": "\n      \n    "
          },
          {
            "name": "Vedangi Pathak",
            "#text": "\n      \n    "
          },
          {
            "name": "Hamed Mohammadbagherpoor",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18348v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18348v1",
      "title": "Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering",
      "abstract": "AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.",
      "content": null,
      "authors": [
        "Matheus Camilo da Silva",
        "Leonardo Arrighi",
        "Ana Carolina Lorena",
        "Sylvio Barbon Junior"
      ],
      "publishedAt": "2026-02-20T17:01:25Z",
      "categories": [
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18348v1",
        "title": "Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering",
        "updated": "2026-02-20T17:01:25Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18348v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18348v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.",
        "category": {
          "term": "cs.LG",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T17:01:25Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Matheus Camilo da Silva",
            "#text": "\n      \n    "
          },
          {
            "name": "Leonardo Arrighi",
            "#text": "\n      \n    "
          },
          {
            "name": "Ana Carolina Lorena",
            "#text": "\n      \n    "
          },
          {
            "name": "Sylvio Barbon Junior",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18346v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18346v1",
      "title": "Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System",
      "abstract": "In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.",
      "content": null,
      "authors": [
        "Pavithra PM Nair",
        "Preethu Rose Anish"
      ],
      "publishedAt": "2026-02-20T16:57:44Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18346v1",
        "title": "Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System",
        "updated": "2026-02-20T16:57:44Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18346v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18346v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.",
        "category": [
          {
            "term": "cs.CL",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T16:57:44Z",
        "arxiv:primary_category": {
          "term": "cs.CL"
        },
        "arxiv:journal_ref": "AI and Law @ AAAI 2026",
        "author": [
          {
            "name": "Pavithra PM Nair",
            "#text": "\n      \n    "
          },
          {
            "name": "Preethu Rose Anish",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18333v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18333v1",
      "title": "On the \"Induction Bias\" in Sequence Models",
      "abstract": "Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.",
      "content": null,
      "authors": [
        "M. Reza Ebrahimi",
        "Michaël Defferrard",
        "Sunny Panchal",
        "Roland Memisevic"
      ],
      "publishedAt": "2026-02-20T16:39:07Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18333v1",
        "title": "On the \"Induction Bias\" in Sequence Models",
        "updated": "2026-02-20T16:39:07Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18333v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18333v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.CL",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T16:39:07Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "M. Reza Ebrahimi",
            "#text": "\n      \n    "
          },
          {
            "name": "Michaël Defferrard",
            "#text": "\n      \n    "
          },
          {
            "name": "Sunny Panchal",
            "#text": "\n      \n    "
          },
          {
            "name": "Roland Memisevic",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18326v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18326v1",
      "title": "Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning",
      "abstract": "We describe a modern deep learning system that automatically identifies informative contextual examples (\\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \\qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.",
      "content": null,
      "authors": [
        "Tao Wu",
        "Adam Kapelner"
      ],
      "publishedAt": "2026-02-20T16:32:14Z",
      "categories": [
        "cs.CL"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18326v1",
        "title": "Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning",
        "updated": "2026-02-20T16:32:14Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18326v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18326v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "We describe a modern deep learning system that automatically identifies informative contextual examples (\\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \\qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.",
        "category": {
          "term": "cs.CL",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T16:32:14Z",
        "arxiv:comment": "8 pages, 3 figures, 4 tables",
        "arxiv:primary_category": {
          "term": "cs.CL"
        },
        "author": [
          {
            "name": "Tao Wu",
            "#text": "\n      \n    "
          },
          {
            "name": "Adam Kapelner",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18324v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18324v1",
      "title": "PsihoRo: Depression and Anxiety Romanian Text Corpus",
      "abstract": "Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.",
      "content": null,
      "authors": [
        "Alexandra Ciobotaru",
        "Ana-Maria Bucur",
        "Liviu P. Dinu"
      ],
      "publishedAt": "2026-02-20T16:24:23Z",
      "categories": [
        "cs.CL"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18324v1",
        "title": "PsihoRo: Depression and Anxiety Romanian Text Corpus",
        "updated": "2026-02-20T16:24:23Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18324v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18324v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.",
        "category": {
          "term": "cs.CL",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T16:24:23Z",
        "arxiv:comment": "This article was accepted at LREC 2026",
        "arxiv:primary_category": {
          "term": "cs.CL"
        },
        "author": [
          {
            "name": "Alexandra Ciobotaru",
            "#text": "\n      \n    "
          },
          {
            "name": "Ana-Maria Bucur",
            "#text": "\n      \n    "
          },
          {
            "name": "Liviu P. Dinu",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18319v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18319v1",
      "title": "Robo-Saber: Generating and Simulating Virtual Reality Players",
      "abstract": "We present the first motion generation system for playtesting virtual reality (VR) games. Our player model generates VR headset and handheld controller movements from in-game object arrangements, guided by style exemplars and aligned to maximize simulated gameplay score. We train on the large BOXRR-23 dataset and apply our framework on the popular VR game Beat Saber. The resulting model Robo-Saber produces skilled gameplay and captures diverse player behaviors, mirroring the skill levels and movement patterns specified by input style exemplars. Robo-Saber demonstrates promise in synthesizing rich gameplay data for predictive applications and enabling a physics-based whole-body VR playtesting agent.",
      "content": null,
      "authors": [
        "Nam Hee Kim",
        "Jingjing May Liu",
        "Jaakko Lehtinen",
        "Perttu Hämäläinen",
        "James F. O'Brien",
        "Xue Bin Peng"
      ],
      "publishedAt": "2026-02-20T16:19:19Z",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18319v1",
        "title": "Robo-Saber: Generating and Simulating Virtual Reality Players",
        "updated": "2026-02-20T16:19:19Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18319v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18319v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "We present the first motion generation system for playtesting virtual reality (VR) games. Our player model generates VR headset and handheld controller movements from in-game object arrangements, guided by style exemplars and aligned to maximize simulated gameplay score. We train on the large BOXRR-23 dataset and apply our framework on the popular VR game Beat Saber. The resulting model Robo-Saber produces skilled gameplay and captures diverse player behaviors, mirroring the skill levels and movement patterns specified by input style exemplars. Robo-Saber demonstrates promise in synthesizing rich gameplay data for predictive applications and enabling a physics-based whole-body VR playtesting agent.",
        "category": [
          {
            "term": "cs.GR",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.HC",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T16:19:19Z",
        "arxiv:comment": "13 pages, 15 figures. Accepted to Eurographics 2026. Project page: https://robo-saber.github.io/",
        "arxiv:primary_category": {
          "term": "cs.GR"
        },
        "author": [
          {
            "name": "Nam Hee Kim",
            "#text": "\n      \n    "
          },
          {
            "name": "Jingjing May Liu",
            "#text": "\n      \n    "
          },
          {
            "name": "Jaakko Lehtinen",
            "#text": "\n      \n    "
          },
          {
            "name": "Perttu Hämäläinen",
            "#text": "\n      \n    "
          },
          {
            "name": "James F. O'Brien",
            "#text": "\n      \n    "
          },
          {
            "name": "Xue Bin Peng",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18313v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18313v1",
      "title": "Clapeyron Neural Networks for Single-Species Vapor-Liquid Equilibria",
      "abstract": "Machine learning (ML) approaches have shown promising results for predicting molecular properties relevant for chemical process design. However, they are often limited by scarce experimental property data and lack thermodynamic consistency. As such, thermodynamics-informed ML, i.e., incorporating thermodynamic relations into the loss function as regularization term for training, has been proposed. We herein transfer the concept of thermodynamics-informed graph neural networks (GNNs) from the Gibbs-Duhem to the Clapeyron equation, predicting several pure component properties in a multi-task manner, namely: vapor pressure, liquid molar volume, vapor molar volume and enthalpy of vaporization. We find improved prediction accuracy of the Clapeyron-GNN compared to the single-task learning setting, and improved approximation of the Clapeyron equation compared to the purely data-driven multi-task learning setting. In fact, we observe the largest improvement in prediction accuracy for the properties with the lowest availability of data, making our model promising for practical application in data scarce scenarios of chemical engineering practice.",
      "content": null,
      "authors": [
        "Jan Pavšek",
        "Alexander Mitsos",
        "Elvis J. Sim",
        "Jan G. Rittig"
      ],
      "publishedAt": "2026-02-20T16:11:42Z",
      "categories": [
        "physics.chem-ph",
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18313v1",
        "title": "Clapeyron Neural Networks for Single-Species Vapor-Liquid Equilibria",
        "updated": "2026-02-20T16:11:42Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18313v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18313v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Machine learning (ML) approaches have shown promising results for predicting molecular properties relevant for chemical process design. However, they are often limited by scarce experimental property data and lack thermodynamic consistency. As such, thermodynamics-informed ML, i.e., incorporating thermodynamic relations into the loss function as regularization term for training, has been proposed. We herein transfer the concept of thermodynamics-informed graph neural networks (GNNs) from the Gibbs-Duhem to the Clapeyron equation, predicting several pure component properties in a multi-task manner, namely: vapor pressure, liquid molar volume, vapor molar volume and enthalpy of vaporization. We find improved prediction accuracy of the Clapeyron-GNN compared to the single-task learning setting, and improved approximation of the Clapeyron equation compared to the purely data-driven multi-task learning setting. In fact, we observe the largest improvement in prediction accuracy for the properties with the lowest availability of data, making our model promising for practical application in data scarce scenarios of chemical engineering practice.",
        "category": [
          {
            "term": "physics.chem-ph",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T16:11:42Z",
        "arxiv:primary_category": {
          "term": "physics.chem-ph"
        },
        "author": [
          {
            "name": "Jan Pavšek",
            "#text": "\n      \n    "
          },
          {
            "name": "Alexander Mitsos",
            "#text": "\n      \n    "
          },
          {
            "name": "Elvis J. Sim",
            "#text": "\n      \n    "
          },
          {
            "name": "Jan G. Rittig",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18308v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18308v1",
      "title": "JPmHC Dynamical Isometry via Orthogonal Hyper-Connections",
      "abstract": "Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.",
      "content": null,
      "authors": [
        "Biswa Sengupta",
        "Jinhua Wang",
        "Leo Brunswic"
      ],
      "publishedAt": "2026-02-20T16:06:01Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18308v1",
        "title": "JPmHC Dynamical Isometry via Orthogonal Hyper-Connections",
        "updated": "2026-02-20T16:06:01Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18308v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18308v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T16:06:01Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Biswa Sengupta",
            "#text": "\n      \n    "
          },
          {
            "name": "Jinhua Wang",
            "#text": "\n      \n    "
          },
          {
            "name": "Leo Brunswic",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18307v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18307v1",
      "title": "VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean",
      "abstract": "Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.",
      "content": null,
      "authors": [
        "Yutong Xin",
        "Qiaochu Chen",
        "Greg Durrett",
        "Işil Dillig"
      ],
      "publishedAt": "2026-02-20T16:05:06Z",
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.LG",
        "cs.PL"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18307v1",
        "title": "VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean",
        "updated": "2026-02-20T16:05:06Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18307v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18307v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.",
        "category": [
          {
            "term": "cs.SE",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.CL",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.PL",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T16:05:06Z",
        "arxiv:primary_category": {
          "term": "cs.SE"
        },
        "author": [
          {
            "name": "Yutong Xin",
            "#text": "\n      \n    "
          },
          {
            "name": "Qiaochu Chen",
            "#text": "\n      \n    "
          },
          {
            "name": "Greg Durrett",
            "#text": "\n      \n    "
          },
          {
            "name": "Işil Dillig",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18306v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18306v1",
      "title": "ReqElicitGym: An Evaluation Environment for Interview Competence in Conversational Requirements Elicitation",
      "abstract": "With the rapid improvement of LLMs' coding capabilities, the bottleneck of LLM-based automated software development is shifting from generating correct code to eliciting users' requirements. Despite growing interest, the interview competence of LLMs in conversational requirements elicitation remains fully underexplored. Existing evaluations often depend on a few scenarios, real user interaction, and subjective human scoring, which hinders systematic and quantitative comparison. To address these challenges, we propose ReqElicitGym, an interactive and automatic evaluation environment for assessing interview competence in conversational requirements elicitation. Specifically, ReqElicitGym introduces a new evaluation dataset and designs both an interactive oracle user and a task evaluator. The dataset contains 101 website requirements elicitation scenarios spanning 10 application types. Both the oracle user and the task evaluator achieve high agreement with real users and expert judgment. Using our ReqElicitGym, any automated conversational requirements elicitation approach (e.g., LLM-based agents) can be evaluated in a reproducible and quantitative manner through interaction with the environment. Based on our ReqElicitGym, we conduct a systematic empirical study on seven representative LLMs, and the results show that current LLMs still exhibit limited interview competence in uncovering implicit requirements. Particularly, they elicit less than half of the users' implicit requirements, and their effective elicitation questions often emerge in later turns of the dialogue. Besides, we found LLMs can elicit interaction and content implicit requirements, but consistently struggle with style-related requirements. We believe ReqElicitGym will facilitate the evaluation and development of automated conversational requirements elicitation.",
      "content": null,
      "authors": [
        "Dongming Jin",
        "Zhi Jin",
        "Zheng Fang",
        "Linyu Li",
        "XiaoTian Yang",
        "Yuanpeng He",
        "Xiaohong Chen"
      ],
      "publishedAt": "2026-02-20T16:02:13Z",
      "categories": [
        "cs.SE"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18306v1",
        "title": "ReqElicitGym: An Evaluation Environment for Interview Competence in Conversational Requirements Elicitation",
        "updated": "2026-02-20T16:02:13Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18306v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18306v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "With the rapid improvement of LLMs' coding capabilities, the bottleneck of LLM-based automated software development is shifting from generating correct code to eliciting users' requirements. Despite growing interest, the interview competence of LLMs in conversational requirements elicitation remains fully underexplored. Existing evaluations often depend on a few scenarios, real user interaction, and subjective human scoring, which hinders systematic and quantitative comparison. To address these challenges, we propose ReqElicitGym, an interactive and automatic evaluation environment for assessing interview competence in conversational requirements elicitation. Specifically, ReqElicitGym introduces a new evaluation dataset and designs both an interactive oracle user and a task evaluator. The dataset contains 101 website requirements elicitation scenarios spanning 10 application types. Both the oracle user and the task evaluator achieve high agreement with real users and expert judgment. Using our ReqElicitGym, any automated conversational requirements elicitation approach (e.g., LLM-based agents) can be evaluated in a reproducible and quantitative manner through interaction with the environment. Based on our ReqElicitGym, we conduct a systematic empirical study on seven representative LLMs, and the results show that current LLMs still exhibit limited interview competence in uncovering implicit requirements. Particularly, they elicit less than half of the users' implicit requirements, and their effective elicitation questions often emerge in later turns of the dialogue. Besides, we found LLMs can elicit interaction and content implicit requirements, but consistently struggle with style-related requirements. We believe ReqElicitGym will facilitate the evaluation and development of automated conversational requirements elicitation.",
        "category": {
          "term": "cs.SE",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T16:02:13Z",
        "arxiv:comment": "22page, 7 figures",
        "arxiv:primary_category": {
          "term": "cs.SE"
        },
        "author": [
          {
            "name": "Dongming Jin",
            "#text": "\n      \n    "
          },
          {
            "name": "Zhi Jin",
            "#text": "\n      \n    "
          },
          {
            "name": "Zheng Fang",
            "#text": "\n      \n    "
          },
          {
            "name": "Linyu Li",
            "#text": "\n      \n    "
          },
          {
            "name": "XiaoTian Yang",
            "#text": "\n      \n    "
          },
          {
            "name": "Yuanpeng He",
            "#text": "\n      \n    "
          },
          {
            "name": "Xiaohong Chen",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18304v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18304v1",
      "title": "FeatureBleed: Inferring Private Enriched Attributes From Sparsity-Optimized AI Accelerators",
      "abstract": "Backend enrichment is now widely deployed in sensitive domains such as product recommendation pipelines, healthcare, and finance, where models are trained on confidential data and retrieve private features whose values influence inference behavior while remaining hidden from the API caller. This paper presents the first hardware-level backend retrieval data-stealing attack, showing that accelerator optimizations designed for performance can directly undermine data confidentiality and bypass state-of-the-art privacy defenses.\n  Our attack, FEATUREBLEED, exploits zero-skipping in AI accelerators to infer private backend-retrieved features solely through end-to-end timing, without relying on power analysis, DVFS manipulation, or shared-cache side channels. We evaluate FEATUREBLEED on three datasets spanning medical and non-medical domains: Texas-100X (clinical records), OrganAMNIST (medical imaging), and Census-19 (socioeconomic data). We further evaluate FEATUREBLEED across three hardware backends (Intel AVX, Intel AMX, and NVIDIA A100) and three model architectures (DNNs, CNNs, and hybrid CNN-MLP pipelines), demonstrating that the leakage generalizes across CPU and GPU accelerators, data modalities, and application domains, with an adversarial advantage of up to 98.87 percentage points.\n  Finally, we identify the root cause of the leakage as sparsity-driven zero-skipping in modern hardware. We quantify the privacy-performance-power trade-off: disabling zero-skipping increases Intel AMX per-operation energy by up to 25 percent and incurs 100 percent performance overhead. We propose a padding-based defense that masks timing leakage by equalizing responses to the worst-case execution time, achieving protection with only 7.24 percent average performance overhead and no additional power cost.",
      "content": null,
      "authors": [
        "Darsh Asher",
        "Farshad Dizani",
        "Joshua Kalyanapu",
        "Rosario Cammarota",
        "Aydin Aysu",
        "Samira Mirbagher Ajorpaz"
      ],
      "publishedAt": "2026-02-20T16:01:16Z",
      "categories": [
        "cs.CR"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18304v1",
        "title": "FeatureBleed: Inferring Private Enriched Attributes From Sparsity-Optimized AI Accelerators",
        "updated": "2026-02-20T16:01:16Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18304v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18304v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Backend enrichment is now widely deployed in sensitive domains such as product recommendation pipelines, healthcare, and finance, where models are trained on confidential data and retrieve private features whose values influence inference behavior while remaining hidden from the API caller. This paper presents the first hardware-level backend retrieval data-stealing attack, showing that accelerator optimizations designed for performance can directly undermine data confidentiality and bypass state-of-the-art privacy defenses.\n  Our attack, FEATUREBLEED, exploits zero-skipping in AI accelerators to infer private backend-retrieved features solely through end-to-end timing, without relying on power analysis, DVFS manipulation, or shared-cache side channels. We evaluate FEATUREBLEED on three datasets spanning medical and non-medical domains: Texas-100X (clinical records), OrganAMNIST (medical imaging), and Census-19 (socioeconomic data). We further evaluate FEATUREBLEED across three hardware backends (Intel AVX, Intel AMX, and NVIDIA A100) and three model architectures (DNNs, CNNs, and hybrid CNN-MLP pipelines), demonstrating that the leakage generalizes across CPU and GPU accelerators, data modalities, and application domains, with an adversarial advantage of up to 98.87 percentage points.\n  Finally, we identify the root cause of the leakage as sparsity-driven zero-skipping in modern hardware. We quantify the privacy-performance-power trade-off: disabling zero-skipping increases Intel AMX per-operation energy by up to 25 percent and incurs 100 percent performance overhead. We propose a padding-based defense that masks timing leakage by equalizing responses to the worst-case execution time, achieving protection with only 7.24 percent average performance overhead and no additional power cost.",
        "category": {
          "term": "cs.CR",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T16:01:16Z",
        "arxiv:comment": "4 pages, 3 figures, 3 tables, Journal :- IEEE CAL",
        "arxiv:primary_category": {
          "term": "cs.CR"
        },
        "author": [
          {
            "name": "Darsh Asher",
            "#text": "\n      \n    "
          },
          {
            "name": "Farshad Dizani",
            "#text": "\n      \n    "
          },
          {
            "name": "Joshua Kalyanapu",
            "#text": "\n      \n    "
          },
          {
            "name": "Rosario Cammarota",
            "#text": "\n      \n    "
          },
          {
            "name": "Aydin Aysu",
            "#text": "\n      \n    "
          },
          {
            "name": "Samira Mirbagher Ajorpaz",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18301v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18301v1",
      "title": "On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction",
      "abstract": "Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for \"imposing\" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.",
      "content": null,
      "authors": [
        "Ivan Bondarenko",
        "Egor Palkin",
        "Fedor Tikunov"
      ],
      "publishedAt": "2026-02-20T15:54:10Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18301v1",
        "title": "On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction",
        "updated": "2026-02-20T15:54:10Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18301v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18301v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for \"imposing\" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.CL",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T15:54:10Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Ivan Bondarenko",
            "#text": "\n      \n    "
          },
          {
            "name": "Egor Palkin",
            "#text": "\n      \n    "
          },
          {
            "name": "Fedor Tikunov",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18297v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18297v1",
      "title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory",
      "abstract": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.",
      "content": null,
      "authors": [
        "Usman Anwar",
        "Tim Bakker",
        "Dana Kianfar",
        "Cristina Pinneri",
        "Christos Louizos"
      ],
      "publishedAt": "2026-02-20T15:50:30Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.IT"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18297v1",
        "title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory",
        "updated": "2026-02-20T15:50:30Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18297v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18297v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.CL",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.IT",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T15:50:30Z",
        "arxiv:comment": "First two authors contributed equally",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Usman Anwar",
            "#text": "\n      \n    "
          },
          {
            "name": "Tim Bakker",
            "#text": "\n      \n    "
          },
          {
            "name": "Dana Kianfar",
            "#text": "\n      \n    "
          },
          {
            "name": "Cristina Pinneri",
            "#text": "\n      \n    "
          },
          {
            "name": "Christos Louizos",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18292v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18292v1",
      "title": "Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers",
      "abstract": "Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.",
      "content": null,
      "authors": [
        "Xiaotong Ji",
        "Rasul Tutunov",
        "Matthieu Zimmer",
        "Haitham Bou-Ammar"
      ],
      "publishedAt": "2026-02-20T15:38:16Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18292v1",
        "title": "Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers",
        "updated": "2026-02-20T15:38:16Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18292v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18292v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T15:38:16Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Xiaotong Ji",
            "#text": "\n      \n    "
          },
          {
            "name": "Rasul Tutunov",
            "#text": "\n      \n    "
          },
          {
            "name": "Matthieu Zimmer",
            "#text": "\n      \n    "
          },
          {
            "name": "Haitham Bou-Ammar",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18291v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18291v1",
      "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies",
      "abstract": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.",
      "content": null,
      "authors": [
        "Zhuoran Li",
        "Hai Zhong",
        "Xun Wang",
        "Qingxin Xia",
        "Lihua Zhang",
        "Longbo Huang"
      ],
      "publishedAt": "2026-02-20T15:38:02Z",
      "categories": [
        "cs.AI"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18291v1",
        "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies",
        "updated": "2026-02-20T15:38:02Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18291v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18291v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.",
        "category": {
          "term": "cs.AI",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T15:38:02Z",
        "arxiv:primary_category": {
          "term": "cs.AI"
        },
        "author": [
          {
            "name": "Zhuoran Li",
            "#text": "\n      \n    "
          },
          {
            "name": "Hai Zhong",
            "#text": "\n      \n    "
          },
          {
            "name": "Xun Wang",
            "#text": "\n      \n    "
          },
          {
            "name": "Qingxin Xia",
            "#text": "\n      \n    "
          },
          {
            "name": "Lihua Zhang",
            "#text": "\n      \n    "
          },
          {
            "name": "Longbo Huang",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18288v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18288v1",
      "title": "A Topology-Aware Positive Sample Set Construction and Feature Optimization Method in Implicit Collaborative Filtering",
      "abstract": "Negative sampling strategies are widely used in implicit collaborative filtering to address issues like data sparsity and class imbalance. However, these methods often introduce false negatives, hindering the model's ability to accurately learn users' latent preferences. To mitigate this problem, existing methods adjust the negative sampling distribution based on statistical features from model training or the hardness of negative samples. Nevertheless, these methods face two key limitations: (1) over-reliance on the model's current representation capabilities; (2) failure to leverage the potential of false negatives as latent positive samples to guide model learning of user preferences more accurately. To address the above issues, we propose a Topology-aware Positive Sample Set Construction and Feature Optimization method (TPSC-FO). First, we design a simple topological community-aware false negative identification (FNI) method and observe that topological community structures in interaction networks can effectively identify false negatives. Motivated by this, we develop a topology-aware positive sample set construction module. This module employs a differential community detection strategy to capture topological community structures in implicit feedback, coupled with personalized noise filtration to reliably identify false negatives and convert them into positive samples. Additionally, we introduce a neighborhood-guided feature optimization module that refines positive sample features by incorporating neighborhood features in the embedding space, effectively mitigating noise in the positive samples. Extensive experiments on five real-world datasets and two synthetic datasets validate the effectiveness of TPSC-FO.",
      "content": null,
      "authors": [
        "Jiayi Wu",
        "Zhengyu Wu",
        "Xunkai Li",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "publishedAt": "2026-02-20T15:35:48Z",
      "categories": [
        "cs.IR"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18288v1",
        "title": "A Topology-Aware Positive Sample Set Construction and Feature Optimization Method in Implicit Collaborative Filtering",
        "updated": "2026-02-20T15:35:48Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18288v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18288v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Negative sampling strategies are widely used in implicit collaborative filtering to address issues like data sparsity and class imbalance. However, these methods often introduce false negatives, hindering the model's ability to accurately learn users' latent preferences. To mitigate this problem, existing methods adjust the negative sampling distribution based on statistical features from model training or the hardness of negative samples. Nevertheless, these methods face two key limitations: (1) over-reliance on the model's current representation capabilities; (2) failure to leverage the potential of false negatives as latent positive samples to guide model learning of user preferences more accurately. To address the above issues, we propose a Topology-aware Positive Sample Set Construction and Feature Optimization method (TPSC-FO). First, we design a simple topological community-aware false negative identification (FNI) method and observe that topological community structures in interaction networks can effectively identify false negatives. Motivated by this, we develop a topology-aware positive sample set construction module. This module employs a differential community detection strategy to capture topological community structures in implicit feedback, coupled with personalized noise filtration to reliably identify false negatives and convert them into positive samples. Additionally, we introduce a neighborhood-guided feature optimization module that refines positive sample features by incorporating neighborhood features in the embedding space, effectively mitigating noise in the positive samples. Extensive experiments on five real-world datasets and two synthetic datasets validate the effectiveness of TPSC-FO.",
        "category": {
          "term": "cs.IR",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T15:35:48Z",
        "arxiv:primary_category": {
          "term": "cs.IR"
        },
        "author": [
          {
            "name": "Jiayi Wu",
            "#text": "\n      \n    "
          },
          {
            "name": "Zhengyu Wu",
            "#text": "\n      \n    "
          },
          {
            "name": "Xunkai Li",
            "#text": "\n      \n    "
          },
          {
            "name": "Rong-Hua Li",
            "#text": "\n      \n    "
          },
          {
            "name": "Guoren Wang",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18287v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18287v1",
      "title": "Green by Design: Constraint-Based Adaptive Deployment in the Cloud Continuum",
      "abstract": "The environmental sustainability of Information Technology (IT) has emerged as a critical concern, driven by the need to reduce both energy consumption and greenhouse gas (GHG) emissions. In the context of cloud-native applications deployed across the cloud-edge continuum, this challenge translates into identifying energy-efficient deployment strategies that consider not only the computational demands of application components but also the environmental impact of the nodes on which they are executed. Generating deployment plans that account for these dynamic factors is non-trivial, due to fluctuations in application behaviour and variations in the carbon intensity of infrastructure nodes. In this paper, we present an approach for the automatic generation of deployment plans guided by green constraints. These constraints are derived from a continuous analysis of energy consumption patterns, inter-component communication, and the environmental characteristics of the underlying infrastructure. This paper introduces a methodology and architecture for the generation of a set of green-aware constraints that inform the scheduler to produce environmentally friendly deployment plans. We demonstrate how these constraints can be automatically learned and updated over time using monitoring data, enabling adaptive, energy-aware orchestration. The proposed approach is validated through realistic deployment scenarios of a cloud-native application, showcasing its effectiveness in reducing energy usage and associated emissions.",
      "content": null,
      "authors": [
        "Andrea D'Iapico",
        "Monica Vitali"
      ],
      "publishedAt": "2026-02-20T15:33:37Z",
      "categories": [
        "cs.DC"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18287v1",
        "title": "Green by Design: Constraint-Based Adaptive Deployment in the Cloud Continuum",
        "updated": "2026-02-20T15:33:37Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18287v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18287v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "The environmental sustainability of Information Technology (IT) has emerged as a critical concern, driven by the need to reduce both energy consumption and greenhouse gas (GHG) emissions. In the context of cloud-native applications deployed across the cloud-edge continuum, this challenge translates into identifying energy-efficient deployment strategies that consider not only the computational demands of application components but also the environmental impact of the nodes on which they are executed. Generating deployment plans that account for these dynamic factors is non-trivial, due to fluctuations in application behaviour and variations in the carbon intensity of infrastructure nodes. In this paper, we present an approach for the automatic generation of deployment plans guided by green constraints. These constraints are derived from a continuous analysis of energy consumption patterns, inter-component communication, and the environmental characteristics of the underlying infrastructure. This paper introduces a methodology and architecture for the generation of a set of green-aware constraints that inform the scheduler to produce environmentally friendly deployment plans. We demonstrate how these constraints can be automatically learned and updated over time using monitoring data, enabling adaptive, energy-aware orchestration. The proposed approach is validated through realistic deployment scenarios of a cloud-native application, showcasing its effectiveness in reducing energy usage and associated emissions.",
        "category": {
          "term": "cs.DC",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T15:33:37Z",
        "arxiv:primary_category": {
          "term": "cs.DC"
        },
        "author": [
          {
            "name": "Andrea D'Iapico",
            "#text": "\n      \n    "
          },
          {
            "name": "Monica Vitali",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18285v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18285v1",
      "title": "Detecting PowerShell-based Fileless Cryptojacking Attacks Using Machine Learning",
      "abstract": "With the emergence of remote code execution (RCE) vulnerabilities in ubiquitous libraries and advanced social engineering techniques, threat actors have started conducting widespread fileless cryptojacking attacks. These attacks have become effective with stealthy techniques based on PowerShell-based exploitation in Windows OS environments. Even if attacks are detected and malicious scripts removed, processes may remain operational on victim endpoints, creating a significant challenge for detection mechanisms. In this paper, we conducted an experimental study with a collected dataset on detecting PowerShell-based fileless cryptojacking scripts. The results showed that Abstract Syntax Tree (AST)-based fine-tuned CodeBERT achieved a high recall rate, proving the importance of the use of AST integration and fine-tuned pre-trained models for programming language.",
      "content": null,
      "authors": [
        "Said Varlioglu",
        "Nelly Elsayed",
        "Murat Ozer",
        "Zag ElSayed",
        "John M. Emmert"
      ],
      "publishedAt": "2026-02-20T15:32:15Z",
      "categories": [
        "cs.CR"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18285v1",
        "title": "Detecting PowerShell-based Fileless Cryptojacking Attacks Using Machine Learning",
        "updated": "2026-02-20T15:32:15Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18285v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18285v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "With the emergence of remote code execution (RCE) vulnerabilities in ubiquitous libraries and advanced social engineering techniques, threat actors have started conducting widespread fileless cryptojacking attacks. These attacks have become effective with stealthy techniques based on PowerShell-based exploitation in Windows OS environments. Even if attacks are detected and malicious scripts removed, processes may remain operational on victim endpoints, creating a significant challenge for detection mechanisms. In this paper, we conducted an experimental study with a collected dataset on detecting PowerShell-based fileless cryptojacking scripts. The results showed that Abstract Syntax Tree (AST)-based fine-tuned CodeBERT achieved a high recall rate, proving the importance of the use of AST integration and fine-tuned pre-trained models for programming language.",
        "category": {
          "term": "cs.CR",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T15:32:15Z",
        "arxiv:comment": "30 papges, Under Review",
        "arxiv:primary_category": {
          "term": "cs.CR"
        },
        "author": [
          {
            "name": "Said Varlioglu",
            "#text": "\n      \n    "
          },
          {
            "name": "Nelly Elsayed",
            "#text": "\n      \n    "
          },
          {
            "name": "Murat Ozer",
            "#text": "\n      \n    "
          },
          {
            "name": "Zag ElSayed",
            "#text": "\n      \n    "
          },
          {
            "name": "John M. Emmert",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18283v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18283v1",
      "title": "HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation",
      "abstract": "Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.",
      "content": null,
      "authors": [
        "Lei Xin",
        "Yuhao Zheng",
        "Ke Cheng",
        "Changjiang Jiang",
        "Zifan Zhang",
        "Fanhu Zeng"
      ],
      "publishedAt": "2026-02-20T15:11:40Z",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18283v1",
        "title": "HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation",
        "updated": "2026-02-20T15:11:40Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18283v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18283v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.",
        "category": [
          {
            "term": "cs.IR",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T15:11:40Z",
        "arxiv:comment": "Preprint",
        "arxiv:primary_category": {
          "term": "cs.IR"
        },
        "author": [
          {
            "name": "Lei Xin",
            "#text": "\n      \n    "
          },
          {
            "name": "Yuhao Zheng",
            "#text": "\n      \n    "
          },
          {
            "name": "Ke Cheng",
            "#text": "\n      \n    "
          },
          {
            "name": "Changjiang Jiang",
            "#text": "\n      \n    "
          },
          {
            "name": "Zifan Zhang",
            "#text": "\n      \n    "
          },
          {
            "name": "Fanhu Zeng",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18277v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18277v1",
      "title": "PRISM: Parallel Reward Integration with Symmetry for MORL",
      "abstract": "This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\\% over the baseline and up to 32\\% over the oracle. The code is at \\href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.",
      "content": null,
      "authors": [
        "Finn van der Knaap",
        "Kejiang Qian",
        "Zheng Xu",
        "Fengxiang He"
      ],
      "publishedAt": "2026-02-20T15:02:42Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18277v1",
        "title": "PRISM: Parallel Reward Integration with Symmetry for MORL",
        "updated": "2026-02-20T15:02:42Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18277v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18277v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\\% over the baseline and up to 32\\% over the oracle. The code is at \\href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.",
        "category": [
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "stat.ML",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T15:02:42Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Finn van der Knaap",
            "#text": "\n      \n    "
          },
          {
            "name": "Kejiang Qian",
            "#text": "\n      \n    "
          },
          {
            "name": "Zheng Xu",
            "#text": "\n      \n    "
          },
          {
            "name": "Fengxiang He",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18270v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18270v1",
      "title": "Many Tools, Few Exploitable Vulnerabilities: A Survey of 246 Static Code Analyzers for Security",
      "abstract": "Static security analysis is a widely used technique for detecting software vulnerabilities across a wide range of weaknesses, application domains, and programming languages. While prior work surveyed static analyzes for specific weaknesses or application domains, no overview of the entire security landscape exists. We present a systematic literature review of 246 static security analyzers concerning their targeted vulnerabilities, application domains, analysis techniques, evaluation methods, and limitations. We observe that most analyzers focus on a limited set of weaknesses, that the vulnerabilities they detect are rarely exploitable, and that evaluations use custom benchmarks that are too small to enable robust assessment.",
      "content": null,
      "authors": [
        "Kevin Hermann",
        "Sven Peldszus",
        "Thorsten Berger"
      ],
      "publishedAt": "2026-02-20T14:52:18Z",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18270v1",
        "title": "Many Tools, Few Exploitable Vulnerabilities: A Survey of 246 Static Code Analyzers for Security",
        "updated": "2026-02-20T14:52:18Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18270v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18270v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Static security analysis is a widely used technique for detecting software vulnerabilities across a wide range of weaknesses, application domains, and programming languages. While prior work surveyed static analyzes for specific weaknesses or application domains, no overview of the entire security landscape exists. We present a systematic literature review of 246 static security analyzers concerning their targeted vulnerabilities, application domains, analysis techniques, evaluation methods, and limitations. We observe that most analyzers focus on a limited set of weaknesses, that the vulnerabilities they detect are rarely exploitable, and that evaluations use custom benchmarks that are too small to enable robust assessment.",
        "category": [
          {
            "term": "cs.CR",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.SE",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T14:52:18Z",
        "arxiv:primary_category": {
          "term": "cs.CR"
        },
        "author": [
          {
            "name": "Kevin Hermann",
            "#text": "\n      \n    "
          },
          {
            "name": "Sven Peldszus",
            "#text": "\n      \n    "
          },
          {
            "name": "Thorsten Berger",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18266v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18266v1",
      "title": "A Probabilistic Framework for LLM-Based Model Discovery",
      "abstract": "Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.",
      "content": null,
      "authors": [
        "Stefan Wahl",
        "Raphaela Schenk",
        "Ali Farnoud",
        "Jakob H. Macke",
        "Daniel Gedon"
      ],
      "publishedAt": "2026-02-20T14:49:53Z",
      "categories": [
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18266v1",
        "title": "A Probabilistic Framework for LLM-Based Model Discovery",
        "updated": "2026-02-20T14:49:53Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18266v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18266v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.",
        "category": {
          "term": "cs.LG",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T14:49:53Z",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Stefan Wahl",
            "#text": "\n      \n    "
          },
          {
            "name": "Raphaela Schenk",
            "#text": "\n      \n    "
          },
          {
            "name": "Ali Farnoud",
            "#text": "\n      \n    "
          },
          {
            "name": "Jakob H. Macke",
            "#text": "\n      \n    "
          },
          {
            "name": "Daniel Gedon",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18262v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18262v1",
      "title": "Simplifying Outcomes of Language Model Component Analyses with ELIA",
      "abstract": "While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.",
      "content": null,
      "authors": [
        "Aaron Louis Eidt",
        "Nils Feldhus"
      ],
      "publishedAt": "2026-02-20T14:45:27Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18262v1",
        "title": "Simplifying Outcomes of Language Model Component Analyses with ELIA",
        "updated": "2026-02-20T14:45:27Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18262v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18262v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.",
        "category": [
          {
            "term": "cs.CL",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.LG",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T14:45:27Z",
        "arxiv:comment": "EACL 2026 System Demonstrations. GitHub: https://github.com/aaron0eidt/ELIA",
        "arxiv:primary_category": {
          "term": "cs.CL"
        },
        "author": [
          {
            "name": "Aaron Louis Eidt",
            "#text": "\n      \n    "
          },
          {
            "name": "Nils Feldhus",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18253v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18253v1",
      "title": "MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data",
      "abstract": "Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.",
      "content": null,
      "authors": [
        "Xabier de Zuazo",
        "Vincenzo Verbeni",
        "Eva Navas",
        "Ibon Saratxaga",
        "Mathieu Bourguignon",
        "Nicola Molinaro"
      ],
      "publishedAt": "2026-02-20T14:39:50Z",
      "categories": [
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18253v1",
        "title": "MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data",
        "updated": "2026-02-20T14:39:50Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18253v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18253v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.",
        "category": {
          "term": "cs.LG",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T14:39:50Z",
        "arxiv:comment": "6 pages, 3 figures, 3 tables, submitted to Interspeech 2026",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": [
          {
            "name": "Xabier de Zuazo",
            "#text": "\n      \n    "
          },
          {
            "name": "Vincenzo Verbeni",
            "#text": "\n      \n    "
          },
          {
            "name": "Eva Navas",
            "#text": "\n      \n    "
          },
          {
            "name": "Ibon Saratxaga",
            "#text": "\n      \n    "
          },
          {
            "name": "Mathieu Bourguignon",
            "#text": "\n      \n    "
          },
          {
            "name": "Nicola Molinaro",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18252v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18252v1",
      "title": "On the Adversarial Robustness of Discrete Image Tokenizers",
      "abstract": "Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.",
      "content": null,
      "authors": [
        "Rishika Bhagwatkar",
        "Irina Rish",
        "Nicolas Flammarion",
        "Francesco Croce"
      ],
      "publishedAt": "2026-02-20T14:39:17Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18252v1",
        "title": "On the Adversarial Robustness of Discrete Image Tokenizers",
        "updated": "2026-02-20T14:39:17Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18252v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18252v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.",
        "category": [
          {
            "term": "cs.CV",
            "scheme": "http://arxiv.org/schemas/atom"
          },
          {
            "term": "cs.AI",
            "scheme": "http://arxiv.org/schemas/atom"
          }
        ],
        "published": "2026-02-20T14:39:17Z",
        "arxiv:primary_category": {
          "term": "cs.CV"
        },
        "author": [
          {
            "name": "Rishika Bhagwatkar",
            "#text": "\n      \n    "
          },
          {
            "name": "Irina Rish",
            "#text": "\n      \n    "
          },
          {
            "name": "Nicolas Flammarion",
            "#text": "\n      \n    "
          },
          {
            "name": "Francesco Croce",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18250v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18250v1",
      "title": "Variational Distributional Neuron",
      "abstract": "We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This \"contraction\" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze \"collapse\" modes and the conditions for a \"living neuron\", then extend the contribution over time via autoregressive priors over the latent, per unit.",
      "content": null,
      "authors": [
        "Yves Ruffenach"
      ],
      "publishedAt": "2026-02-20T14:35:53Z",
      "categories": [
        "cs.LG"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18250v1",
        "title": "Variational Distributional Neuron",
        "updated": "2026-02-20T14:35:53Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18250v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18250v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This \"contraction\" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze \"collapse\" modes and the conditions for a \"living neuron\", then extend the contribution over time via autoregressive priors over the latent, per unit.",
        "category": {
          "term": "cs.LG",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T14:35:53Z",
        "arxiv:comment": "29 pages, 7 figures. Code available at GitHub (link in paper)",
        "arxiv:primary_category": {
          "term": "cs.LG"
        },
        "author": {
          "name": "Yves Ruffenach",
          "#text": "\n      \n    "
        },
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    },
    {
      "id": "arxiv:2602.18249v1",
      "source": "arxiv",
      "sourceType": "paper",
      "url": "https://arxiv.org/abs/2602.18249v1",
      "title": "Dual-Tree LLM-Enhanced Negative Sampling for Implicit Collaborative Filtering",
      "abstract": "Negative sampling is a pivotal technique in implicit collaborative filtering (CF) recommendation, enabling efficient and effective training by contrasting observed interactions with sampled unobserved ones.\n  Recently, large language models (LLMs) have shown promise in recommender systems; however, research on LLM-empowered negative sampling remains underexplored.\n  Existing methods heavily rely on textual information and task-specific fine-tuning, limiting practical applicability.\n  To address this limitation, we propose a text-free and fine-tuning-free Dual-Tree LLM-enhanced Negative Sampling method (DTL-NS).\n  It consists of two modules: (i) an offline false negative identification module that leverages hierarchical index trees to transform collaborative structural and latent semantic information into structured item-ID encodings for LLM inference, enabling accurate identification of false negatives; and (ii) a multi-view hard negative sampling module that combines user-item preference scores with item-item hierarchical similarities from these encodings to mine high-quality hard negatives, thus improving models' discriminative ability.\n  Extensive experiments demonstrate the effectiveness of DTL-NS. For example, on the Amazon-sports dataset, DTL-NS outperforms the strongest baseline by 10.64% and 19.12% in Recall@20 and NDCG@20, respectively.\n  Moreover, DTL-NS can be integrated into various implicit CF models and negative sampling methods, consistently enhancing their performance.",
      "content": null,
      "authors": [
        "Jiayi Wu",
        "Zhengyu Wu",
        "Xunkai Li",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "publishedAt": "2026-02-20T14:32:41Z",
      "categories": [
        "cs.IR"
      ],
      "language": null,
      "raw": {
        "id": "http://arxiv.org/abs/2602.18249v1",
        "title": "Dual-Tree LLM-Enhanced Negative Sampling for Implicit Collaborative Filtering",
        "updated": "2026-02-20T14:32:41Z",
        "link": [
          {
            "href": "https://arxiv.org/abs/2602.18249v1",
            "rel": "alternate",
            "type": "text/html"
          },
          {
            "href": "https://arxiv.org/pdf/2602.18249v1",
            "rel": "related",
            "type": "application/pdf",
            "title": "pdf"
          }
        ],
        "summary": "Negative sampling is a pivotal technique in implicit collaborative filtering (CF) recommendation, enabling efficient and effective training by contrasting observed interactions with sampled unobserved ones.\n  Recently, large language models (LLMs) have shown promise in recommender systems; however, research on LLM-empowered negative sampling remains underexplored.\n  Existing methods heavily rely on textual information and task-specific fine-tuning, limiting practical applicability.\n  To address this limitation, we propose a text-free and fine-tuning-free Dual-Tree LLM-enhanced Negative Sampling method (DTL-NS).\n  It consists of two modules: (i) an offline false negative identification module that leverages hierarchical index trees to transform collaborative structural and latent semantic information into structured item-ID encodings for LLM inference, enabling accurate identification of false negatives; and (ii) a multi-view hard negative sampling module that combines user-item preference scores with item-item hierarchical similarities from these encodings to mine high-quality hard negatives, thus improving models' discriminative ability.\n  Extensive experiments demonstrate the effectiveness of DTL-NS. For example, on the Amazon-sports dataset, DTL-NS outperforms the strongest baseline by 10.64% and 19.12% in Recall@20 and NDCG@20, respectively.\n  Moreover, DTL-NS can be integrated into various implicit CF models and negative sampling methods, consistently enhancing their performance.",
        "category": {
          "term": "cs.IR",
          "scheme": "http://arxiv.org/schemas/atom"
        },
        "published": "2026-02-20T14:32:41Z",
        "arxiv:primary_category": {
          "term": "cs.IR"
        },
        "author": [
          {
            "name": "Jiayi Wu",
            "#text": "\n      \n    "
          },
          {
            "name": "Zhengyu Wu",
            "#text": "\n      \n    "
          },
          {
            "name": "Xunkai Li",
            "#text": "\n      \n    "
          },
          {
            "name": "Rong-Hua Li",
            "#text": "\n      \n    "
          },
          {
            "name": "Guoren Wang",
            "#text": "\n      \n    "
          }
        ],
        "#text": "\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  "
      }
    }
  ],
  "config": {},
  "run": {}
}
